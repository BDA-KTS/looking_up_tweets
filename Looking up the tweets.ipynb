{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "HojYv92k8CxA",
   "metadata": {
    "id": "HojYv92k8CxA"
   },
   "source": [
    "# Semantic search over social media posts (tweets)\n",
    "\n",
    "The method implements semantic search over a collection of social media posts e.g., tweets, and returns the most similar posts to the search query. It uses a pretrained language model (embeddings) to determine the posts closest to the query text using cosine similarity and returns the ranked results.  \n",
    "\n",
    "The script is divided into two main sections:\n",
    "\n",
    "__1. Getting the Document Embeddings:__ This section processes tweets and converts them into numerical embeddings.\n",
    "\n",
    "__2. Looking Up Tweets:__ Here, we use cosine similarity to find tweets that are most similar to the user-provided search term.\n",
    "\n",
    "We use Twitter samples downloaded from the NLTK library. You can download it here:\n",
    "```\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "nzP8hJqA8CxG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2062,
     "status": "ok",
     "timestamp": 1715317383835,
     "user": {
      "displayName": "Fakhri Momeni",
      "userId": "05371720958134709651"
     },
     "user_tz": -120
    },
    "id": "nzP8hJqA8CxG",
    "outputId": "501a63df-b946-466e-b85e-426a821c48e6"
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "\n",
    "from utils import (cosine_similarity, get_dict,\n",
    "                   process_tweet)\n",
    "from os import getcwd\n",
    "\n",
    "import w4_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "u8qGtpN38CxH",
   "metadata": {
    "id": "u8qGtpN38CxH"
   },
   "outputs": [],
   "source": [
    "# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
    "filePath = f\"{getcwd()}/tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5WHFxJeO8CxH",
   "metadata": {
    "id": "5WHFxJeO8CxH"
   },
   "outputs": [],
   "source": [
    "all_tweets = twitter_samples.strings('tweets.20150430-223406.json')#all_positive_tweets + all_negative_tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iqbAWxNiD4Jr",
   "metadata": {
    "id": "iqbAWxNiD4Jr"
   },
   "source": [
    "\n",
    "## 1 - Getting the Document Embeddings\n",
    "\n",
    "###  The Word Embeddings Data for English Words\n",
    "\n",
    "The full dataset for English embeddings is about 3.64 gigabytes. To prevent the workspace from\n",
    "crashing, we've extracted a subset of the embeddings for the words that you'll\n",
    "use in this Tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "QFvb-KYuENiS",
   "metadata": {
    "id": "QFvb-KYuENiS"
   },
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"./data/en_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GQ8FFDW58CxI",
   "metadata": {
    "id": "GQ8FFDW58CxI"
   },
   "source": [
    "\n",
    "\n",
    "### Bag-of-words (BOW) Document Models\n",
    "Text documents are sequences of words.\n",
    "* The ordering of words makes a difference. For example, sentences \"Apple pie is\n",
    "better than pepperoni pizza.\" and \"Pepperoni pizza is better than apple pie\"\n",
    "have opposite meanings due to the word ordering.\n",
    "* However, for some applications, ignoring the order of words can allow\n",
    "us to train an efficient and still effective model.\n",
    "* This approach is called Bag-of-words document model.\n",
    "\n",
    "### Document Embeddings\n",
    "* Document embedding is created by summing up the embeddings of all words\n",
    "in the document.\n",
    "* If we don't know the embedding of some word, we can ignore that word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geJAOUKx8CxI",
   "metadata": {
    "id": "geJAOUKx8CxI"
   },
   "source": [
    "<a name=\"1-1-4\"></a>\n",
    "\n",
    "### Function 'get_document_embedding'\n",
    "* The function `get_document_embedding()` encodes entire document as a \"document\" embedding.\n",
    "* It takes in a document (as a string) and a dictionary, `en_embeddings`\n",
    "* It processes the document, and looks up the corresponding embedding of each word.\n",
    "* It then sums them up and returns the sum of all word vectors of that processed tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9hUI39rP8CxJ",
   "metadata": {
    "id": "9hUI39rP8CxJ"
   },
   "outputs": [],
   "source": [
    "# UNQ_C12 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_document_embedding(tweet, en_embeddings, process_tweet=process_tweet):\n",
    "    '''\n",
    "    Input:\n",
    "        - tweet: a string\n",
    "        - en_embeddings: a dictionary of word embeddings\n",
    "    Output:\n",
    "        - doc_embedding: sum of all word embeddings in the tweet\n",
    "    '''\n",
    "    doc_embedding = np.zeros(300)\n",
    "\n",
    "    # process the document into a list of words (process the tweet)\n",
    "    processed_doc = process_tweet(tweet)\n",
    "    for word in processed_doc:\n",
    "        # add the word embedding to the running total for the document embedding\n",
    "        doc_embedding += en_embeddings.get(word, np.zeros(300))\n",
    "\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EKzzc5-y8CxL",
   "metadata": {
    "id": "EKzzc5-y8CxL"
   },
   "source": [
    "<a name=\"1-1-5\"></a>\n",
    "### Function 'get_document_vecs'\n",
    "\n",
    "#### Store all document vectors into a dictionary\n",
    "Now, let's store all the tweet embeddings into a dictionary.\n",
    "Implement `get_document_vecs()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2VyV5UAI8CxL",
   "metadata": {
    "id": "2VyV5UAI8CxL"
   },
   "outputs": [],
   "source": [
    "# UNQ_C14 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_document_vecs(all_docs, en_embeddings, get_document_embedding=get_document_embedding):\n",
    "    '''\n",
    "    Input:\n",
    "        - all_docs: list of strings - all tweets in our dataset.\n",
    "        - en_embeddings: dictionary with words as the keys and their embeddings as the values.\n",
    "    Output:\n",
    "        - document_vec_matrix: matrix of tweet embeddings.\n",
    "        - ind2Doc_dict: dictionary with indices of tweets in vecs as keys and their embeddings as the values.\n",
    "    '''\n",
    "\n",
    "    # the dictionary's key is an index (integer) that identifies a specific tweet\n",
    "    # the value is the document embedding for that document\n",
    "    ind2Doc_dict = {}\n",
    "\n",
    "    # this is list that will store the document vectors\n",
    "    document_vec_l = []\n",
    "\n",
    "    for i, doc in enumerate(all_docs):\n",
    "\n",
    "        # get the document embedding of the tweet\n",
    "        doc_embedding = get_document_embedding(doc,en_embeddings)\n",
    "\n",
    "        # save the document embedding into the ind2Tweet dictionary at index i\n",
    "        ind2Doc_dict[i] = doc_embedding\n",
    "\n",
    "        # append the document embedding to the list of document vectors\n",
    "        document_vec_l.append(doc_embedding)\n",
    "\n",
    "\n",
    "    # convert the list of document vectors into a 2D array (each row is a document vector)\n",
    "    document_vec_matrix = np.vstack(document_vec_l)\n",
    "\n",
    "    return document_vec_matrix, ind2Doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0S07QJvK8CxL",
   "metadata": {
    "id": "0S07QJvK8CxL"
   },
   "outputs": [],
   "source": [
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "Lap_DXGQ8CxM",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Lap_DXGQ8CxM",
    "outputId": "10436fb4-1972-470b-d8e0-66d8823a828c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 20000\n",
      "shape of document_vecs (20000, 300)\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C15 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "print(f\"length of dictionary {len(ind2Tweet)}\")\n",
    "print(f\"shape of document_vecs {document_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8G8lDqU8CxM",
   "metadata": {
    "id": "c8G8lDqU8CxM"
   },
   "source": [
    "\n",
    "## 2 - Looking up the Tweets\n",
    "\n",
    "Now you have a vector of dimension (m,d) where `m` is the number of tweets\n",
    "(10,000) and `d` is the dimension of the embeddings (300).  Now you\n",
    "will input a tweet, and use cosine similarity to see which tweet in our\n",
    "corpus is similar to your tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "MeA6N9ep8CxM",
   "metadata": {
    "id": "MeA6N9ep8CxM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top similar tweets:\n",
      "Tweet: @hugorifkind Audience - good. Mili - bad. Clegg - a bit sad. Cam - unscathed (Similarity: 0.7288986447264361)\n",
      "Tweet: @bootheghost @gaarbage oh UKIP *playful tutting* sad thing is that they're popular though jesus christ (Similarity: 0.6744912436671723)\n",
      "Tweet: @Bad_Braminski Why do you think it's sad? I'm curious as one of those who wanted to go it alone. And for this election, my vote for SNP isnt (Similarity: 0.6620227394305912)\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'I am sad'\n",
    "process_tweet(my_tweet)\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)\n",
    "cosine_similarities = cosine_similarity(document_vecs, tweet_embedding)\n",
    "\n",
    "#The number of returned tweets\n",
    "N = 3\n",
    "top_indices = np.argsort(cosine_similarities)[-N:][::-1]\n",
    "# Display the top N most similar tweets\n",
    "print(\"Top similar tweets:\")\n",
    "for idx in top_indices:\n",
    "    print(f\"Tweet: {all_tweets[idx]} (Similarity: {cosine_similarities[idx]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7eb6fa-83ae-48ef-aab6-3c12fef611f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
