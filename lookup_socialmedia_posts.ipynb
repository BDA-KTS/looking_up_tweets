{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "HojYv92k8CxA",
   "metadata": {
    "id": "HojYv92k8CxA"
   },
   "source": [
    "# Semantic search over social media posts (tweets)\n",
    "\n",
    "The method implements semantic search over a collection of social media posts e.g., tweets, and returns the most similar posts to the search query. It uses a pretrained language model (embeddings) to determine the posts closest to the query text using cosine similarity and returns the ranked results.  \n",
    "\n",
    "The script is divided into two main sections:\n",
    "\n",
    "__1. Getting the Document Embeddings:__ This section processes social media posts and converts them into numerical embeddings.\n",
    "\n",
    "__2. Looking Up posts:__ Here, we use cosine similarity to find social media posts that are most similar to the user-provided search query.\n",
    "\n",
    "*Some utility functionalities regarding data loading, preprocessing, tokenization are in the `utils.py` file.*\n",
    "\n",
    "We use Twitter samples downloaded from the NLTK library to demonstrate this method. You can download it here:\n",
    "\n",
    "```\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f47f530-3d08-4548-aa46-163c67fc7553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal (utils.py) and external resources\n",
    "from utils import (load_data, clean_posts, tokenize_posts, \n",
    "cosine_similarity, read_configurations, write_output)\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n"
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3775078-5731-4b47-91e1-a624ad3e990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For predictable random numbers in reuse\n",
    "import random\n",
    "random.seed(13)\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2eb4259-0c4e-4e8c-a6bd-4e4990e24b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ifpreprocess': True,\n",
       " 'top-k': 5,\n",
       " 'input_query_filepath': '/data/input_queries.txt',\n",
       " 'output_filepath': '/data/output.json',\n",
       " 'posts_filepath': '/corpora/tweets.20150430-223406.json'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configurations = read_configurations(\"/config.json\")\n",
    "configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38699fac-a1c4-464f-8439-44e84e8e9633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['social media', 'women', 'election'],\n",
       " ['hopeless for tmr :(',\n",
       "  \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\",\n",
       "  '@Hegelbon That heart sliding into the waste basket. :('])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read input queries\n",
    "\n",
    "\n",
    "ls_input_queries, ls_posts = load_data(configurations['input_query_filepath'], \n",
    "                            configurations['posts_filepath'])\n",
    "ls_input_queries[:3], ls_posts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iqbAWxNiD4Jr",
   "metadata": {
    "id": "iqbAWxNiD4Jr"
   },
   "source": [
    "\n",
    "## 1 - Getting the Document Embeddings\n",
    "\n",
    "###  The Word Embeddings Data for English Words\n",
    "\n",
    "The full dataset for English embeddings is about 3.64 gigabytes. To prevent the workspace from\n",
    "crashing, we've extracted a subset of the embeddings for the words that you'll\n",
    "use in this Tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83648f22-d092-42df-bef3-0c10508e5750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "en_embeddings = pickle.load(open(\"./embeddings/en_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GQ8FFDW58CxI",
   "metadata": {
    "id": "GQ8FFDW58CxI"
   },
   "source": [
    "\n",
    "\n",
    "### Bag-of-words (BOW) Document Models\n",
    "Text documents are sequences of words.\n",
    "* The ordering of words makes a difference. For example, sentences \"Apple pie is\n",
    "better than pepperoni pizza.\" and \"Pepperoni pizza is better than apple pie\"\n",
    "have opposite meanings due to the word ordering.\n",
    "* However, for some applications, ignoring the order of words can allow\n",
    "us to train an efficient and still effective model. *In this method, we are averaging the word vectors in a post i.e., losing their position related information*\n",
    "* This approach is called Bag-of-words document model.\n",
    "\n",
    "### Document Embeddings\n",
    "* Document embedding is created by summing up the embeddings of all words\n",
    "in the document.\n",
    "* If we don't know the embedding of some word, we can ignore that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17fb7d8-caca-4400-a802-d2255516ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess social media posts by removing urls, hashtags, stickers and other unwanted patterns.\n",
    "\n",
    "if configurations[\"ifpreprocess\"]:\n",
    "    posts = clean_posts(ls_posts)\n",
    "\n",
    "# Tokenize, stem and return clean tokens\n",
    "tokenized_posts = tokenize_posts(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geJAOUKx8CxI",
   "metadata": {
    "id": "geJAOUKx8CxI"
   },
   "source": [
    "<a name=\"1-1-4\"></a>\n",
    "\n",
    "### Function 'get_document_embedding'\n",
    "* The function `get_document_embedding()` encodes entire document as a \"document\" embedding.\n",
    "* It takes in a document (as a string) and a dictionary, `en_embeddings`\n",
    "* It processes the document, and looks up the corresponding embedding of each word.\n",
    "* It then sums them up and returns the sum of all word vectors of that processed tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9hUI39rP8CxJ",
   "metadata": {
    "id": "9hUI39rP8CxJ"
   },
   "outputs": [],
   "source": [
    "#Computer doc embedding vector i.e., average of all its word embeddings\n",
    "def compute_doc_embedding(post):\n",
    "    doc_embedding = np.zeros(300)\n",
    "    for token in post:\n",
    "        # add the word embedding to the running total for the document embedding\n",
    "        doc_embedding += en_embeddings.get(token, np.zeros(300))\n",
    "        doc_embedding = np.divide(doc_embedding, len(post))\n",
    "    return doc_embedding\n",
    "\n",
    "# This method reads a social media posts anc returns its embedded vector i.e., the average of embedded vectors of all its words\n",
    "def vectorize_posts(posts, en_embeddings):\n",
    "    '''\n",
    "    Input:\n",
    "        - tweet: a string\n",
    "        - en_embeddings: a dictionary of word embeddings\n",
    "    Output:\n",
    "        - doc_embedding: sum of all word embeddings in the tweet\n",
    "    '''\n",
    "\n",
    "    # the dictionary's key is an index (integer) that identifies a specific tweet\n",
    "    # the value is the document embedding for that document\n",
    "    ind2Doc_dict = {}\n",
    "\n",
    "    # this is list that will store the document vectors\n",
    "    document_vec_l = []\n",
    "    \n",
    "    posts_embeddings = []\n",
    "    i = 0\n",
    "    for post in posts:\n",
    "        doc_embedding = compute_doc_embedding(post)\n",
    "\n",
    "        # save the document embedding into the ind2Tweet dictionary at index i\n",
    "        ind2Doc_dict[i] = doc_embedding\n",
    "        i += 1\n",
    "        # append the document embedding to the list of document vectors\n",
    "        document_vec_l.append(doc_embedding)\n",
    "\n",
    "        # convert the list of document vectors into a 2D array (each row is a document vector)\n",
    "        document_vec_matrix = np.vstack(document_vec_l)\n",
    "\n",
    "    return document_vec_matrix, ind2Doc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EKzzc5-y8CxL",
   "metadata": {
    "id": "EKzzc5-y8CxL"
   },
   "source": [
    "<a name=\"1-1-5\"></a>\n",
    "### Function 'Vectorize_posts'\n",
    "\n",
    "#### Store all document vectors into a dictionary\n",
    "Now, let's store all the posts embeddings into a dictionary.\n",
    "Implement `vectorize_posts()`\n",
    "\n",
    "The following cell computes embedding (Posts x vector) matirx having each post represented with the standard 300 size vector. It may take *5-10mins* for *20,000 posts* on regular PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60e3ee01-00ff-46d0-a0f3-6e474b3cbc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized documents into their 300vector embeddings\n",
    "# The word vectors are averaged for each document\n",
    "\n",
    "posts_vec_matrix, ind2Doc_dict = vectorize_posts(tokenized_posts,en_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b971ce9e-a591-406d-af54-53e7b3fa092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 5000\n",
      "shape of document_vecs (5000, 300)\n"
     ]
    }
   ],
   "source": [
    "# ind2Doc dictionary and matrix of posts vectors generated\n",
    "\n",
    "print(f\"length of dictionary {len(ind2Doc_dict)}\")\n",
    "print(f\"shape of document_vecs {posts_vec_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8G8lDqU8CxM",
   "metadata": {
    "id": "c8G8lDqU8CxM"
   },
   "source": [
    "\n",
    "## 2 - Looking up the Posts\n",
    "\n",
    "Now you have a vector of dimension (m,d) where `m` is the number of posts\n",
    "(20,000) and `d` is the dimension of the embeddings (300).  \n",
    "\n",
    "Now we will calculate post similarities for the query inputs using cosine similarity over the entire posts vector matix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70228993-5690-4d14-a5fb-1cafe28ca992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['social', 'media'], ['women'], ['election']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess queries.\n",
    "\n",
    "if configurations[\"ifpreprocess\"]:\n",
    "    ls_input_queries = clean_posts(ls_input_queries)\n",
    "\n",
    "# Tokenize, stem and return clean tokens\n",
    "tokenized_queries = tokenize_posts(ls_input_queries)\n",
    "tokenized_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f8166e3-5ac6-4784-b793-eeab074a7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get top K similar posts for all query items\n",
    "\n",
    "query_posts_similarities = {}\n",
    "for query in tokenized_queries:\n",
    "    query_embedding = compute_doc_embedding(query)\n",
    "    cosine_score = cosine_similarity(posts_vec_matrix, query_embedding)\n",
    "    top_indices = np.argsort(cosine_score)[-configurations[\"top-k\"]:][::-1]\n",
    "    query_str = ' '.join(query)\n",
    "    query_posts_similarities[query_str] = []\n",
    "    top_posts = []\n",
    "    for idx in top_indices:\n",
    "        top_posts.append({'post ID': str(idx),\n",
    "                          'post text': ls_posts[idx], \n",
    "                          \"sim score\": str(cosine_score[idx])})\n",
    "    query_posts_similarities[query_str] = top_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2e536f1-dcbd-4df1-b00c-13278c9f870c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"social media\": [{\"post ID\": \"4991\", \"post text\": \"@louanndavies Completely agree. The press won\\'t :(\", \"sim score\": \"0.5183101761642593\"}, {\"post ID\": \"496\", \"post text\": \"NOOoo @PTTBath  I\\'ve just heard the news :(\", \"sim score\": \"0.45883602205460944\"}, {\"post ID\": \"1622\", \"post text\": \"Ehdar pegea panga dosto :(\\\\n\\\\nOpen link nd read this news\\\\n\\\\nhttp://t.co/QjNXZbJyA0 http://t.co/je1E4gboSw\", \"sim score\": \"0.4555587424847405\"}, {\"post ID\": \"2697\", \"post text\": \"@AdityaRajKaul really thought you were one good journo.. But the lure of the gang I see is very strong.. Sad to see you too twisting news :(\", \"sim score\": \"0.45409698342828103\"}, {\"post ID\": \"2847\", \"post text\": \"Good news on the Boaz Myhill front but not on takeover news :-( #wba\", \"sim score\": \"0.45404673999207423\"}], \"women\": [{\"post ID\": \"4036\", \"post text\": \"Last night was one of the worst night :( I pretty sure this Albanian women cursed me\", \"sim score\": \"0.9999968437940444\"}, {\"post ID\": \"3227\", \"post text\": \"@WforWoman A9) It would be a sad world for women ! :(\\\\n#WSaleLove\", \"sim score\": \"0.9991169322191223\"}, {\"post ID\": \"4691\", \"post text\": \"@JayMcGuiness @JayMcGuiness :-( please notice men\", \"sim score\": \"0.7331858239617141\"}, {\"post ID\": \"1211\", \"post text\": \"@EmperorJepp yeah, they are just the next prey :( poor girls\", \"sim score\": \"0.6388692740244029\"}, {\"post ID\": \"162\", \"post text\": \"sigh i feel bad for our girls :(\", \"sim score\": \"0.6339138615584305\"}], \"election\": [{\"post ID\": \"4192\", \"post text\": \"Sucks how I\\'m Gona miss Chellos party :(\", \"sim score\": \"0.3692610628447474\"}, {\"post ID\": \"3674\", \"post text\": \"@Virgin_TrainsEC unfortunately the only seats left were in coach K so we\\'ll have to party quietly :( ^DH\", \"sim score\": \"0.3691799056548693\"}, {\"post ID\": \"4325\", \"post text\": \"Man right now, I wish someone would bring me food to my office. I\\'m starving. :(\", \"sim score\": \"0.36682136271397303\"}, {\"post ID\": \"875\", \"post text\": \"@Rorington95 did they manage to sort your ticket issue mate? Nobody picked up when i rang the box office :(\", \"sim score\": \"0.3641142720674552\"}, {\"post ID\": \"877\", \"post text\": \"Not 12 hours after my sister-in-law installed that new @DionoUSA seat we bought her she was rear-ended! everyone\\'s fine but seat is trash :(\", \"sim score\": \"0.34857574604371855\"}]}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(query_posts_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3ca3d7d-998f-4603-ada7-87d13a149d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output in json format\n",
    "write_output(configurations['output_filepath'], json.dumps(query_posts_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7266f7-df59-496c-9e43-af251abbcc0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
