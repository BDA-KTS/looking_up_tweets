{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "HojYv92k8CxA",
   "metadata": {
    "id": "HojYv92k8CxA"
   },
   "source": [
    "# Semantic search over social media posts (tweets)\n",
    "\n",
    "This method performs a semantic search on a collection of social media posts, such as tweets, and identifies the posts most similar to a given search query. It leverages a pretrained language model (embeddings) to calculate semantic similarity using cosine similarity and provides ranked results based on relevance.\n",
    "\n",
    "The script is divided into these sections:\n",
    "\n",
    "__1. Environment Setup and Dependencies:__ This section imports all necessary utility functions and configurations.\n",
    "\n",
    "__2. Data Loading and Configuration:__  Here, the input query is set, and the dataset (social media posts) is loaded for the search process.\n",
    "\n",
    "__3. Getting the Document Embeddings:__\n",
    " This section processes the social media posts and transforms them into numerical embeddings.\n",
    "\n",
    "__4. Semantic Search through the Posts:__ Using cosine similarity, the script identifies posts most similar to the user-provided search query.\n",
    "\n",
    "__5. Output:__ The search results are displayed and saved in a JSON file for further analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30307edc-a521-4cd9-b094-846fe35c2693",
   "metadata": {},
   "source": [
    " ## 1. Environment Setup and Dependencies\n",
    "*Some utility functionalities regarding data loading, preprocessing, and tokenization are in the `utils.py` file.*\n",
    "\n",
    "We use Twitter samples downloaded from the NLTK library to demonstrate this method. You can download it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ab1e70-cc5e-497f-a72b-5787e32c426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\momenifi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd242ee-bbda-446e-8bbd-cd6df3cf0562",
   "metadata": {},
   "source": [
    "Now we import import internal (utils.py) and external resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f47f530-3d08-4548-aa46-163c67fc7553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal (utils.py) and external resources\n",
    "from utils import (load_data, clean_posts, tokenize_posts, \n",
    "cosine_similarity, read_configurations, write_output)\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4f083-2b1d-49fe-b4d8-089922fbebae",
   "metadata": {},
   "source": [
    "We predefine random seeds to ensure reproducibility of results across executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3775078-5731-4b47-91e1-a624ad3e990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For predictable random numbers in reuse\n",
    "import random\n",
    "random.seed(13)\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac47f7-594e-43d1-a112-a508f5a0c4e4",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Configuration\n",
    "We now load the configurations from `/config.json`. This file defines the paths for the dataset and input data, the location for saving output results, and additional parameters to customize the method's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2eb4259-0c4e-4e8c-a6bd-4e4990e24b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ifpreprocess': True,\n",
       " 'top-k': 5,\n",
       " 'input_query_filepath': '/data/input_queries.txt',\n",
       " 'output_filepath': '/data/output.json',\n",
       " 'posts_filepath': '/corpora/tweets.20150430-223406.json'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configurations = read_configurations(\"/config.json\")\n",
    "configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0cf0a8-08ab-4c62-a148-d907a2b662db",
   "metadata": {},
   "source": [
    "Now, you can set your own input query directly. If you leave the `ls_input_queries` variable empty, the method will read the query from the input file defined in the configuration. The posts will be loaded from the file specified in the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38699fac-a1c4-464f-8439-44e84e8e9633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input queries\n",
    " \n",
    "default_input_queries, ls_posts = load_data(configurations['input_query_filepath'], \n",
    "                            configurations['posts_filepath'])\n",
    "ls_input_queries = \"\"\n",
    "\n",
    "if not ls_input_queries:\n",
    "    ls_input_queries = default_input_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iqbAWxNiD4Jr",
   "metadata": {
    "id": "iqbAWxNiD4Jr"
   },
   "source": [
    "\n",
    "## 3. Getting the Document Embeddings\n",
    "\n",
    "###  The Word Embeddings Data for English Words\n",
    "\n",
    "The full dataset for English embeddings is about 3.64 gigabytes. To prevent the workspace from\n",
    "crashing, we've extracted a subset of the embeddings for the words that you'll\n",
    "use in this Tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83648f22-d092-42df-bef3-0c10508e5750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "en_embeddings = pickle.load(open(\"./embeddings/en_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GQ8FFDW58CxI",
   "metadata": {
    "id": "GQ8FFDW58CxI"
   },
   "source": [
    "\n",
    "\n",
    "### Bag-of-words (BOW) Document Models\n",
    "Text documents are sequences of words.\n",
    "* The ordering of words makes a difference. For example, sentences \"Apple pie is\n",
    "better than pepperoni pizza.\" and \"Pepperoni pizza is better than apple pie\"\n",
    "have opposite meanings due to the word ordering.\n",
    "* However, for some applications, ignoring the order of words can allow\n",
    "us to train an efficient and still effective model. *In this method, we are averaging the word vectors in a post i.e., losing their position related information*\n",
    "* This approach is called Bag-of-words document model.\n",
    "\n",
    "### Document Embeddings\n",
    "* Document embedding is created by summing up the embeddings of all words\n",
    "in the document.\n",
    "* If we don't know the embedding of some word, we can ignore that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a17fb7d8-caca-4400-a802-d2255516ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess social media posts by removing urls, hashtags, stickers and other unwanted patterns.\n",
    "\n",
    "if configurations[\"ifpreprocess\"]:\n",
    "    posts = clean_posts(ls_posts)\n",
    "\n",
    "# Tokenize, stem and return clean tokens\n",
    "tokenized_posts = tokenize_posts(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geJAOUKx8CxI",
   "metadata": {
    "id": "geJAOUKx8CxI"
   },
   "source": [
    "<a name=\"1-1-4\"></a>\n",
    "\n",
    "### Function 'get_document_embedding'\n",
    "* The function `get_document_embedding()` encodes entire document as a \"document\" embedding.\n",
    "* It takes in a document (as a string) and a dictionary, `en_embeddings`\n",
    "* It processes the document, and looks up the corresponding embedding of each word.\n",
    "* It then sums them up and returns the sum of all word vectors of that processed tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9hUI39rP8CxJ",
   "metadata": {
    "id": "9hUI39rP8CxJ"
   },
   "outputs": [],
   "source": [
    "#Computer doc embedding vector i.e., average of all its word embeddings\n",
    "def compute_doc_embedding(post):\n",
    "    doc_embedding = np.zeros(300)\n",
    "    for token in post:\n",
    "        # add the word embedding to the running total for the document embedding\n",
    "        doc_embedding += en_embeddings.get(token, np.zeros(300))\n",
    "        doc_embedding = np.divide(doc_embedding, len(post))\n",
    "    return doc_embedding\n",
    "\n",
    "# This method reads a social media posts anc returns its embedded vector i.e., the average of embedded vectors of all its words\n",
    "def vectorize_posts(posts, en_embeddings):\n",
    "    '''\n",
    "    Input:\n",
    "        - tweet: a string\n",
    "        - en_embeddings: a dictionary of word embeddings\n",
    "    Output:\n",
    "        - doc_embedding: sum of all word embeddings in the tweet\n",
    "    '''\n",
    "\n",
    "    # the dictionary's key is an index (integer) that identifies a specific tweet\n",
    "    # the value is the document embedding for that document\n",
    "    ind2Doc_dict = {}\n",
    "\n",
    "    # this is list that will store the document vectors\n",
    "    document_vec_l = []\n",
    "    \n",
    "    posts_embeddings = []\n",
    "    i = 0\n",
    "    for post in posts:\n",
    "        doc_embedding = compute_doc_embedding(post)\n",
    "\n",
    "        # save the document embedding into the ind2Tweet dictionary at index i\n",
    "        ind2Doc_dict[i] = doc_embedding\n",
    "        i += 1\n",
    "        # append the document embedding to the list of document vectors\n",
    "        document_vec_l.append(doc_embedding)\n",
    "\n",
    "        # convert the list of document vectors into a 2D array (each row is a document vector)\n",
    "        document_vec_matrix = np.vstack(document_vec_l)\n",
    "\n",
    "    return document_vec_matrix, ind2Doc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EKzzc5-y8CxL",
   "metadata": {
    "id": "EKzzc5-y8CxL"
   },
   "source": [
    "<a name=\"1-1-5\"></a>\n",
    "### Function 'Vectorize_posts'\n",
    "\n",
    "#### Store all document vectors into a dictionary\n",
    "Now, let's store all the posts embeddings into a dictionary.\n",
    "Implement `vectorize_posts()`\n",
    "\n",
    "The following cell computes embedding (Posts x vector) matirx having each post represented with the standard 300 size vector. It may take *5-10mins* for *20,000 posts* on regular PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60e3ee01-00ff-46d0-a0f3-6e474b3cbc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized documents into their 300vector embeddings\n",
    "# The word vectors are averaged for each document\n",
    "\n",
    "posts_vec_matrix, ind2Doc_dict = vectorize_posts(tokenized_posts,en_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b971ce9e-a591-406d-af54-53e7b3fa092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 5000\n",
      "shape of document_vecs (5000, 300)\n"
     ]
    }
   ],
   "source": [
    "# ind2Doc dictionary and matrix of posts vectors generated\n",
    "\n",
    "print(f\"length of dictionary {len(ind2Doc_dict)}\")\n",
    "print(f\"shape of document_vecs {posts_vec_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8G8lDqU8CxM",
   "metadata": {
    "id": "c8G8lDqU8CxM"
   },
   "source": [
    "\n",
    "## 4 - Semantic Search through the Posts\n",
    "\n",
    "Now you have a vector of dimension (m,d) where `m` is the number of posts\n",
    "(20,000) and `d` is the dimension of the embeddings (300).  \n",
    "\n",
    "Now we will calculate post similarities for the query inputs using cosine similarity over the entire posts vector matix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70228993-5690-4d14-a5fb-1cafe28ca992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['social', 'media'], ['women'], ['election']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess queries.\n",
    "\n",
    "if configurations[\"ifpreprocess\"]:\n",
    "    ls_input_queries = clean_posts(ls_input_queries)\n",
    "\n",
    "# Tokenize, stem and return clean tokens\n",
    "tokenized_queries = tokenize_posts(ls_input_queries)\n",
    "tokenized_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f8166e3-5ac6-4784-b793-eeab074a7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get top K similar posts for all query items\n",
    "\n",
    "query_posts_similarities = {}\n",
    "for query in tokenized_queries:\n",
    "    query_embedding = compute_doc_embedding(query)\n",
    "    cosine_score = cosine_similarity(posts_vec_matrix, query_embedding)\n",
    "    top_indices = np.argsort(cosine_score)[-configurations[\"top-k\"]:][::-1]\n",
    "    query_str = ' '.join(query)\n",
    "    query_posts_similarities[query_str] = []\n",
    "    top_posts = []\n",
    "    for idx in top_indices:\n",
    "        top_posts.append({'post ID': str(idx),\n",
    "                          'post text': ls_posts[idx], \n",
    "                          \"sim score\": str(cosine_score[idx])})\n",
    "    query_posts_similarities[query_str] = top_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2e536f1-dcbd-4df1-b00c-13278c9f870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json.dumps(query_posts_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a47948a-b872-44dd-b16c-a5b7371578de",
   "metadata": {},
   "source": [
    "## 5. Output\n",
    "Now we save output in json format and you can read the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3ca3d7d-998f-4603-ada7-87d13a149d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_output(configurations['output_filepath'], json.dumps(query_posts_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a7266f7-df59-496c-9e43-af251abbcc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'social media': [{'post ID': '4991',\n",
       "   'post text': \"@louanndavies Completely agree. The press won't :(\",\n",
       "   'sim score': '0.5183101761642595'},\n",
       "  {'post ID': '496',\n",
       "   'post text': \"NOOoo @PTTBath  I've just heard the news :(\",\n",
       "   'sim score': '0.45883602205460944'},\n",
       "  {'post ID': '1622',\n",
       "   'post text': 'Ehdar pegea panga dosto :(\\n\\nOpen link nd read this news\\n\\nhttp://t.co/QjNXZbJyA0 http://t.co/je1E4gboSw',\n",
       "   'sim score': '0.4555587424847405'},\n",
       "  {'post ID': '2697',\n",
       "   'post text': '@AdityaRajKaul really thought you were one good journo.. But the lure of the gang I see is very strong.. Sad to see you too twisting news :(',\n",
       "   'sim score': '0.45409698342828103'},\n",
       "  {'post ID': '2847',\n",
       "   'post text': 'Good news on the Boaz Myhill front but not on takeover news :-( #wba',\n",
       "   'sim score': '0.4540467399920742'}],\n",
       " 'women': [{'post ID': '4036',\n",
       "   'post text': 'Last night was one of the worst night :( I pretty sure this Albanian women cursed me',\n",
       "   'sim score': '0.9999968437940444'},\n",
       "  {'post ID': '3227',\n",
       "   'post text': '@WforWoman A9) It would be a sad world for women ! :(\\n#WSaleLove',\n",
       "   'sim score': '0.9991169322191223'},\n",
       "  {'post ID': '4691',\n",
       "   'post text': '@JayMcGuiness @JayMcGuiness :-( please notice men',\n",
       "   'sim score': '0.7331858239617141'},\n",
       "  {'post ID': '1211',\n",
       "   'post text': '@EmperorJepp yeah, they are just the next prey :( poor girls',\n",
       "   'sim score': '0.6388692740244029'},\n",
       "  {'post ID': '162',\n",
       "   'post text': 'sigh i feel bad for our girls :(',\n",
       "   'sim score': '0.6339138615584305'}],\n",
       " 'election': [{'post ID': '4192',\n",
       "   'post text': \"Sucks how I'm Gona miss Chellos party :(\",\n",
       "   'sim score': '0.3692610628447474'},\n",
       "  {'post ID': '3674',\n",
       "   'post text': \"@Virgin_TrainsEC unfortunately the only seats left were in coach K so we'll have to party quietly :( ^DH\",\n",
       "   'sim score': '0.3691799056548694'},\n",
       "  {'post ID': '4325',\n",
       "   'post text': \"Man right now, I wish someone would bring me food to my office. I'm starving. :(\",\n",
       "   'sim score': '0.366821362713973'},\n",
       "  {'post ID': '875',\n",
       "   'post text': '@Rorington95 did they manage to sort your ticket issue mate? Nobody picked up when i rang the box office :(',\n",
       "   'sim score': '0.3641142720674552'},\n",
       "  {'post ID': '877',\n",
       "   'post text': \"Not 12 hours after my sister-in-law installed that new @DionoUSA seat we bought her she was rear-ended! everyone's fine but seat is trash :(\",\n",
       "   'sim score': '0.3485757460437185'}]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_posts_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8bcc4-f71e-4016-840b-fb4b454469fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
